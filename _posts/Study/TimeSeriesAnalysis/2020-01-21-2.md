---
title: 2. Stationarity, White Noise, Trend, Autocovariance and Autocorrelation
tags: Study_TimeSeriesAnalysis
aside:
  toc: true
---

# Remarks
본 글은 [한양대학교 이기천 교수님의 시계열분석 강의 2강](https://youtu.be/xhUEIHgbcnc)을 정리한 글입니다.

<!--more-->

---

# 1. Autocovariance / Autocorrelation function
## 1) Autocovariance function (1장 참조)
$\gamma_x(h) = Cov(x_t, x_{t+h})$

## 2) Autocorrelation function
### - Autocorrelation function
$\rho_x(h) = \frac{\gamma_x(h)}{\sqrt{\gamma_x(0)} \sqrt{\gamma_x(0)}} = \frac{\gamma_x(h)}{\gamma_x(0)}$ <br>
- Scale의 영향을 제거하기 위해, autocovariance function $\gamma_x(h)$를 분산($Cov(x_t, x_t) = \gamma_x(0)$)으로 나누어 normalize한 식이다. <br>
- $-1 \leq \rho_x(h) \leq 1$

### - Properties
- $\mid \gamma_x(h) \mid \leq \gamma_x(0)$ <br>
- $\gamma_x(h) = \gamma_x(-h)$ <br>


# 2. Linear models
## 1) Moving average model
### - Linear model followed by moving average of order $q$ ($MA(q)$)
$x_t = a_t - \theta_1 a_{t-1} - \cdots - \theta_q a_{t-q} + \mu$ <br>

- $t$ 시점의 관측치는 $t$ 시점의 white noise와 이전 시점의 white noise들($+ \ or \ -$)과 $\mu$로 구성된다. <br>

### - Check stationarity for $q=1$
$x_t = a_t - \theta a_{t-1} + \mu$ <br>

1. **$E[x_t] = constant$** <br>
$$
\begin{equation}
\begin{aligned}
  E[x_t] &= E[a_t] - \theta E[a_{t-1}] + \mu \\
  &= \mu \ (constant)
\end{aligned}
\end{equation}
$$
<br><br>
2. **$Cov(x_t, x_{t+h})$ depends only $h$** <br>
$$
\begin{equation}
\begin{aligned}
  \gamma(0) &= Cov(a_t - \theta a_{t-1}, a_t - \theta a_{t-1}) \\
  &= Cov(a_t, a_t) - Cov(a_t, \theta a_{t-1}) - Cov(a_t, \theta a_{t-1}) + Cov(\theta a_{t-1}, \theta a_{t-1}) \\
  &= \sigma^2 - 0 - 0 + \theta^2 \sigma^2 \\
  &= (\theta^2 + 1)\sigma^2
\end{aligned}
\end{equation}
$$
$$
\begin{equation}
\begin{aligned}
  \gamma(1) &= Cov(a_t - \theta a_{t-1}, a_{t+1} - \theta a_{t}) \\
  &= Cov(a_t, a_{t+1}) - Cov(a_t, \theta a_t) - Cov(\theta a_{t-1}, a_{t+1}) + Cov(\theta a_{t-1}, \theta a_t) \\
  &= 0 - \theta \sigma^2 - 0 + 0 \\
  &= -\theta \sigma^2
\end{aligned}
\end{equation}
$$
$$
\begin{equation}
\begin{aligned}
  \gamma(h) &= Cov(a_t - \theta a_{t-1}, a_{t+h} - \theta a_{t+h-1})  \quad \cdots \quad h > 1\\
  &= Cov(a_t, a_{t+h}) - Cov(a_t, \theta a_{t+h-1}) - Cov(\theta a_{t-1}, a_{t+h}) + Cov(\theta a_{t-1}, \theta a_{t+h-1}) \\
  &= 0 - 0 - 0 + 0 \\
  &= 0
\end{aligned}
\end{equation}
$$
<br><br>
3. **$Var(x_t) = Var(x_{t+h})$** <br>
$Var(a_t - \theta a_{t-1}) = Var(a_{t+h} - \theta a_{t+h-1})$

# 3. Trend Estimation
## 1) Smoothing with a finite moving average filter
$x_t = m_t + y_t \quad s.t. \ m_t: trend, \ y_t: residual$ <br>
For moving window length $q$ (one-side) <br>
$$
\begin{equation}
\begin{aligned}
  w_t &= \frac{1}{2q + 1} \sum_{j=-q}^q x_{t-j} \\
  &= \frac{1}{2q + 1} \sum_{j=-q}^q m_{t-j} + \frac{1}{2q + 1} \sum_{j=-q}^q y_{t-j} \\
  &\approx \hat{m}_t + 0
\end{aligned}
\end{equation}
$$

- Moving average를 통해 trend $\tilde{m}_t$를 찾을 수 있다. <br>

## 2) Exponential Smoothing
$$
\begin{equation}
\begin{aligned}
  \hat m_1 &= x_1 \\
  \hat m_t &= \alpha x_t + (1 - \alpha)\hat m_{t-1} \\
  &= \hat m_{t-1} + \alpha (x_t - \hat m_{t-1})
\end{aligned}
\end{equation}
$$

- Moving average를 통해 구해지는 trend는 lag가 생겨 뒤늦게 나온다. <br>
따라서, 늦춰진 시점을 맞춰주기 위해 exponential smoothing을 사용할 수 있다.

## 3) Smoothing splines
각 데이터를 부드럽게 이어주는 model인 spline의 기본이 되는 piecewise polynomials(일반적으로 3차, cubic spline)은 점들을 따라가는데만 집중하기 때문에 trend를 놓칠 수 있다. <br>
부드럽게 변하는 trend를 반영하기 위해 spline을 좀 더 부드럽게 만들기 위한 regularization을 추가한 smoothing spline을 사용할 수 있다. <br>

**Piecewise polynomials $f_t$** given by minimizing <br>
$$\sum_{t=1}^n [x_t - f_t]^2 + \lambda \int (f_t'')^2 dt$$
$\lambda$: level(degree) of smoothness (selected by cross-validation)

## 4) Kernel Smoothing
예를 들어, 다음과 같은 moving average model이 너무 단순하다고 생각된다면 <br>
$\hat m_t = \frac{1}{3}x_{t-2} + \frac{1}{3}x_{t-1} + \frac{1}{3}x_{t}$ <br><br>

다음과 같이 time step 마다 다른 weight를 가진 **Kernel smoothing** model을 사용할 수 있다. <br>
$\hat m_t = \sum_{i=n}^n w_i(t) x_i$ <br>
$w_i(t) = \frac{K(\frac{t-i}{b})}{\sum_j K(\frac{t-j}{b})}$ : $i$'th observation's contribution(influence) to $x_t$ <br>
$K(z) = \frac{1}{\sqrt(2\pi)} \exp - \frac{z^2}{2}$ <br>
$b$: Bandwidth (define the variance of kernel function $K(z)$, selected by cross-validation)

## 5) Trend elimination by differencing
$x_t = m_t + y_t$ 에서 OLS($t \to m_t$)를 통해 $\hat m_t = -11.2 + 0.006t$를 얻을 수 있었다. <br>
이에 따라, residual의 추정치 $\hat y_t = x_t - \hat m_t = x_t + 11.2 - 0.006t$ 를 구할 수 있다. 이것이 stationary 하는가를 체크하여 다른 model들을 시작해나간다. <br>

혹은, $x_t - x_{t-1}$ 을 계산함으로써 동일한 작업을 할 수 있다. <br>
$x_t - x_{t-1} = (m_t - m_{t-1}) + (y_t - y_{t-1})$ <br>
만약 trend는 linear하다면, $m_t - m_{t-1}$는 constant가 되어 쉽게 다음 modeling 작업을 진행할 수 있게 된다. <br>

$z_t = x_t - x_{t-1}$는 stationary 할까? <br>
Trend $m_t$가 constant로 사라져버리기 때문에, $y_t$가 stationary하다면 $z_t$ 역시 stationary하다. <br>
