---
title: EM Algorithm
tags: Theory
---

# Remarks
이 글은 [Computer Vision: Models, Learning, and Inference](http://www.computervisionmodels.com/)를 참조하여 작성되었습니다.

<!--more-->

---

## I. Motivation
1) MLE가 closed form solution으로 구해지기 위해선, (log) likelihood가 closed form으로 나와야하지만 그렇지 못한 경우가 많습니다.
2) 그러한 경우, hidden(latent) variable을 추가하는 방법을 사용하면 문제를 비교적 쉽게 풀 수 있습니다.
3) Hidden variable을 도입하여 MLE (혹은 MAP)를 구하는 알고리즘이 바로 **EM algorithm**입니다.

## II. Algorithm
### 1. Likelihood with hidden variable $h$

$$ \begin{equation}
\begin{aligned}
    Pr(\mathbf{x} \mid \theta)
    &= \int Pr(\mathbf{x}, \mathbf{h} \mid \theta) d\mathbf{h} \\
    &= \int Pr(\mathbf{x} \mid \mathbf{h}, \theta) Pr(\mathbf{h} \mid \theta) d\mathbf{h} \\
    &= \int Pr(\mathbf{x} \mid \mathbf{h}, \theta) Pr(\mathbf{h}) d\mathbf{h} \quad \cdots \quad \mathbf{h} \text{ is independent to } \theta
\end{aligned}
\end{equation} $$

### 2. Define a lower bound on log likelihood

$$ \begin{equation}
\begin{aligned}
    \log Pr(\mathbf{x} \mid \theta)
    &= \log \Pi_{i=1}^N Pr(x_i \mid \theta) \\
    &= \Sigma_{i=1}^N \log Pr(x_i \mid \theta) \\
    &= \Sigma_{i=1}^N \log \int Pr(x_i, h_i \mid \theta) dh_i \\
    &= \Sigma_{i=1}^N \log \int q_i(h_i) \frac{Pr(x_i, h_i \mid \theta)}{q_i(h_i)} dh_i \\
    &\geq \Sigma_{i=1}^N \int q_i(h_i) \log \frac{Pr(x_i, h_i \mid \theta)}{q_i(h_i)} dh_i \quad \cdots \quad \text{Jensen's inequality} \\
    &= B[{q_i(h_i)}, \theta]
\end{aligned}
\end{equation} $$


$$
