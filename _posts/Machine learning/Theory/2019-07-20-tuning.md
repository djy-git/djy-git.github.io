---
layout: article
title: Hyperparameter tuning
tags: Theory
sidebar:
  nav: docs-en
---

모델을 선택한 다음 해야할 일은 모델의 hyperparameter를 정하는 **hyperparameter tuning** 과정입니다. <br>

<!--more-->

---

Parameter를 학습시킬 때 **train data**를 사용하여 score를 비교하고 가장 높은 score를 보여준 parameter를 선택하였습니다. 적절한 hyperparameter를 찾을 땐 overfitting을 막기 위해 train data와 분리된 **validation data**를 사용하는데요. Validation data를 나누고 tuning 하는 방법을 크게 2가지로 구분할 수 있습니다. <br>

### 1. <학습 - 예측 - score 계산> 과정의 시간이 오래 걸리는 경우
Deep learning과 같이 많은 양의 데이터를 다루거나 학습시키고 예측하여 score를 계산하는 시간이 오래 걸리는 경우, 처음 data를 나눌 때 서로 격리된 **train / validation / test data** 로 나눕니다. <br>
Train data와 마찬가지로 **validation data에서 발생하는 cost가 test data에서 발생하는 cost와 유사할 것이다** 라는 전제하에 validation data에서 가장 좋은 성능을 보여주는 hyperparameter를 선택할 수 있습니다.


### 2. <학습 - 예측 - score 계산> 과정의 시간이 오래 걸리지 않는 경우
또다른 방법은 먼저 **validation data**를 정해놓지 않고 데이터를 **train / test data**로 나눕니다. Train data를 동일한 크기를 가진 k개의 fold로 분할한 다음, 각각의 fold를 validation data로, 그 외의 (k-1)개의 folds를 train data로 하는 데이터셋 k개를 생각해볼 수 있습니다. 그리고 동일한 hyperparameter를 가진 모델을 k개 만들어 각 데이터셋에 대해서 학습한 결과 score의 평균을 구하고 이 과정을 각 hyperparameter 마다 반복하여 비교할 수 있습니다. 이러한 방법을 **k-fold cross validation**이라고 합니다. <br>
Cross validation은 하나의 hyperparameter를 평가하기 위해 서로 다른 train / validation data에 대한 k개의 모델을 학습시키고 그 평균값으로 평가하기 때문에 hyperparameter의 일반화 성능에 대한 신뢰성이 높을 뿐만 아니라, validation data가 고정되어 있지 않아 train data의 효율성을 극대화하는 방법으로 자주 사용되는 방법입니다.

*sklearn*에서 기본적인 estimator를 모델로 사용한다면 여러가지 종류의 cross validation 함수를 사용할 수 있습니다. <br>
※ `n_jobs=-1` option을 사용하여 CPU 병렬화를 최대화시키도록 합시다! <br>


**1) `cross_val_score()`** <br>
각각의 validation fold에 대한 score를 반환합니다.

{% highlight python linenos %}
from sklearn.model_selection import cross_val_score

reg_scores = cross_val_score(reg, X_train, y_train, scoring='neg_mean_squared_error', cv=10)  # Regressor
clf_scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1)     # Classifier

def display_scores(scores, type=None):
    if type == 'nmse':
        scores = np.sqrt(-scores)
    print('Scores:', scores)
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())

display_scores(reg_scores, 'nmse')
display_scores(clf_scores)
{% endhighlight %}


**2) `cross_val_predict()`** <br>
각각의 validation fold에 대한 예측값을 반환합니다. (unseen data에 대한 예측값이기 때문에 clean predict라고도 부릅니다)

{% highlight python linenos %}
from sklearn.model_selection import cross_val_predict

y_pred = cross_val_predict(clf, X_train, y_train, cv=5, n_jobs=-1)
{% endhighlight %}

**3) `GridSearchCV`** <br>
Grid search를 하여 hyperparameter tuning할 수 있는 class입니다.

{% highlight python linenos %}
from sklearn.model_selection import GridSearchCV

param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
  ]

forest_reg = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)
grid_search.fit(housing_prepared, housing_labels)

grid_search.best_params_
grid_search.best_estimator_
cvres = grid_search.cv_results_
for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):
    print(np.sqrt(-mean_score), params)
pd.DataFrame(grid_search.cv_results_)

feature_importances = grid_search.best_estimator_.feature_importances_  # feature의 중요도를 알 수 있습니다

{% endhighlight %}

**4) `RandomSearchCV`** <br>
Random search를 하여 hyperparameter tuning할 수 있는 class입니다. (scipy.stats의 여러가지 확률분포들을 사용할 수 있습니다)

{% highlight python linenos %}
from sklearn.model_selection import RandomSearchCV
from scipy.stats import randint

param_dist = {
  'n_estimators': randint(low=1, high=200),
  'max_features': randint(low=1, high=8)
}
forest_reg = RandomForestRegressor(random_state=42)
rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_dist, n_iter=10, cv=5,
                                scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)
rnd_search.fit(housing_prepared, housing_labels)

rnd_search.best_params_
rnd_search.best_estimator_
cvres = rnd_search.cv_results_
for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):
    print(np.sqrt(-mean_score), params)
pd.DataFrame(grid_search.cv_results_)

{% endhighlight %}


**5) `validation_curve()`** <br>
train_score와 validation_score를 같이 반환하는 함수입니다.

{% highlight python linenos %}

from sklearn.datasets import load_digits
from sklearn.svm import SVC
from sklearn.model_selection import validation_curve

digits = load_digits()
X, y = digits.data, digits.target
param_range = np.logspace(-6, -1, 10)

%%time
train_scores, test_scores = validation_curve(SVC(), X, y,
      param_name="gamma", param_range=param_range, cv=10, scoring="accuracy", n_jobs=-1)

train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

mpl.rcParams["font.family"] = 'DejaVu Sans'
plt.semilogx(param_range, train_scores_mean, label="Training score", color="r")
plt.fill_between(param_range, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.2, color="r")
plt.semilogx(param_range, test_scores_mean,
             label="Cross-validation score", color="g")
plt.fill_between(param_range, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha=0.2, color="g")
plt.legend(loc="best")
plt.title("Validation Curve with SVM")
plt.xlabel("$\gamma$")
plt.ylabel("Score")
plt.ylim(0.0, 1.1)
plt.show()

{% endhighlight %}

![Image](https://raw.githubusercontent.com/djy-git/djy-git.github.io/master/_posts/assets/val_curve.png){:.border}[^1]


[^1]: [https://datascienceschool.net/view-notebook/ff4b5d491cc34f94aea04baca86fbef8/](https://datascienceschool.net/view-notebook/ff4b5d491cc34f94aea04baca86fbef8/) 참조
