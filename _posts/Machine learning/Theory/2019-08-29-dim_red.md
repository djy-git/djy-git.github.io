---
layout: article
title: Dimensionality Reduction
tags: Theory
sidebar:
  nav: docs-en
---

# Remarks
본 포스팅은 **Hands-On Machine Learning with Scikit-Learn & TensorFlow ([Auérlien Géron](/images/dt_files/https://github.com/ageron/handson-ml), [박해선(역)](/images/dt_files/https://github.com/rickiepark/handson-ml), 한빛미디어)** 를 참고하여 작성되었습니다.

<!--more-->

---

# 1. Curse of Dimensionality
대부분의 데이터들은 굉장히 많은 개수의 feauture들을 가지고 있습니다. 이는 학습을 느리게 할 뿐만 아니라, 좋은 솔루션을 찾기 어렵게 만듭니다. 이런 문제를 **차원의 저주 (curse of dimensionality)**라고 부릅니다. <br><br>

## 고차원에서는 overfitting의 위험이 크다
예를 들어 단위 면적 (1x1) 안에 있는 점을 무작위로 선택한다면, 경계선에서 0.001 이내에 위치할 가능성은 0.4% 밖에 안 되지만, 10,000차원의 초입방체에선 **99.999999%** 보다 커지게 됩니다. 즉, 고차원 초입방체에 있는 대다수의 점은 경계와 매우 가까이 있습니다. <br>

더 심각한 점으론 단위 면적에서 임의의 두 점을 선택했을 때의 평균 거리는 대략 0.52 정도가 되지만, 1,000,000차원에서는 평균 거리가 약 **428.25** 나 되어 대부분의 데이터들이 서로 멀리 떨어져 있습니다. <br>
이 말은, **예측해야할 데이터가 알고 있는 학습 데이터와 멀리 떨어져 있을 가능성이 높다**는 말이기 때문에 저차원에 비해 예측이 더 불안정해지고 overfitting의 위험이 커지는 문제가 발생합니다.


<br>
# 2. Dimensionality Reduction - Projection
거의 모든 데이터들은 모든 차원에 걸쳐 균일하게 퍼져 있지 않지 않고, 고차원 공간 안의 저차원 subspace 근처에 놓여 있습니다. 따라서 적절한 subspace를 찾아 데이터들을 이 위로 투영시키면 데이터를 최대한 보존하면서 차원을 축소시킬 수 있습니다. <br>

가장 대표적인 방법으로 **PCA**가 있습니다.

## 1) PCA (Principal Component Analysis)
데이터에서 가장 분산이 큰 축을 찾아 투영시키는 방법으로, 원본 데이터와 투영된 것 사이의 MSE를 최소화하는 축을 찾는 방법으로도 생각할 수 있습니다.

![png](/images/dim_red_files/pca_1.png)

데이터에서 분산이 최대인 축의 단위벡터를 **principal component (주성분, PC)**라 부르고 **SVD**를 통해 구할 수 있습니다. <br>

---

1. 결국 PCA가 하고자 하는 일은 **(orthogonal) basis를 변환**하는 작업입니다.
$ P \in \mathbb{R^{k \times n}}: (\mathbf{x_1}, \cdots, \mathbf{x_n}) → (\mathbf{y_1}, \cdots, \mathbf{y_k}) $
$$
\begin{aligned}
Y \ ([k, m]) =
\begin{pmatrix}
\\
\mathbf{y^{(1)}}& \cdots & \mathbf{y^{(m)}} \\
\\
\end{pmatrix} =
\begin{pmatrix}
\mathbf{p_{11}}& \cdots & \mathbf{p_{1n}} \\
\vdots & \ddots & \vdots \\
\mathbf{p_{k1}}& \cdots & \mathbf{p_{kn}} \\
\end{pmatrix}
\begin{pmatrix}
\\
\mathbf{x^{(1)}}& \cdots & \mathbf{x^{(m)}} \\
\\
\end{pmatrix}
= PX \ ([k, n] \cdot [n, m])
\end{aligned}
$$

2. 여기서 $P$는 변환된 $Y$의 feature들간의 covariance가 0가 되도록 정해주어야 합니다. <br>
즉, $C_Y$ ($Y$의 covariance matrix)가 diagonal matrix가 되는 $P$를 선택합니다. <br>


---
