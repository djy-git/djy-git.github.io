---
layout: article
title: Dimensionality Reduction
tags: Theory
sidebar:
  nav: docs-en
---

# Remarks
본 포스팅은 **Hands-On Machine Learning with Scikit-Learn & TensorFlow ([Auérlien Géron](/images/dt_files/https://github.com/ageron/handson-ml), [박해선(역)](/images/dt_files/https://github.com/rickiepark/handson-ml), 한빛미디어)** 를 참고하여 작성되었습니다.

<!--more-->

---

# 1. Curse of Dimensionality
대부분의 데이터들은 굉장히 많은 개수의 feauture들을 가지고 있습니다. 이는 학습을 느리게 할 뿐만 아니라, 좋은 솔루션을 찾기 어렵게 만듭니다. 이런 문제를 **차원의 저주 (curse of dimensionality)**라고 부릅니다. <br><br>

## 고차원에서는 overfitting의 위험이 크다
예를 들어 단위 면적 (1x1) 안에 있는 점을 무작위로 선택한다면, 경계선에서 0.001 이내에 위치할 가능성은 0.4% 밖에 안 되지만, 10,000차원의 초입방체에선 **99.999999%** 보다 커지게 됩니다. 즉, 고차원 초입방체에 있는 대다수의 점은 경계와 매우 가까이 있습니다. <br>

더 심각한 점으론 단위 면적에서 임의의 두 점을 선택했을 때의 평균 거리는 대략 0.52 정도가 되지만, 1,000,000차원에서는 평균 거리가 약 **428.25** 나 되어 대부분의 데이터들이 서로 멀리 떨어져 있습니다. <br>
이 말은, **예측해야할 데이터가 알고 있는 학습 데이터와 멀리 떨어져 있을 가능성이 높다**는 말이기 때문에 저차원에 비해 예측이 더 불안정해지고 overfitting의 위험이 커지는 문제가 발생합니다.


<br>
# 2. Dimensionality Reduction - Projection
거의 모든 데이터들은 모든 차원에 걸쳐 균일하게 퍼져 있지 않지 않고, 고차원 공간 안의 저차원 subspace 근처에 놓여 있습니다. 따라서 적절한 subspace를 찾아 데이터들을 이 위로 투영시키면 데이터를 최대한 보존하면서 차원을 축소시킬 수 있습니다. <br>

가장 대표적인 방법으로 **PCA**가 있습니다.

## 1) PCA (Principal Component Analysis)
데이터에서 가장 분산이 큰 축을 찾아 투영시키는 방법으로, 원본 데이터와 투영된 것 사이의 MSE를 최소화하는 축을 찾는 방법으로도 생각할 수 있습니다.

![png](/images/dim_red_files/pca_1.png)

데이터에서 분산이 최대인 축들의 단위벡터를 **principal component (주성분, PC)**라 부르고 다음과 같은 과정을 통해 구할 수 있습니다. <br>

<br>1. 결국 PCA가 하고자 하는 일은 **(orthogonal) basis를 변환**하는 작업입니다. <br>
$ X \in \mathbb{R}^{m \times n}, Y \in \mathbb{R}^{m \times k} $
$ P \in \mathbb{R}^{n \times k}: (\mathbf{x_1}, \cdots, \mathbf{x_n}) → (\mathbf{y_1}, \cdots, \mathbf{y_k}) $
$ Y = XP $

<br>2. 여기서 $P$는 변환된 $Y$의 feature들간의 covariance가 0가 되도록 정해주어야 합니다. (대각화)<br>
즉, **$C_Y$ ($Y$의 covariance matrix)가 diagonal matrix가 되는 $P$를 선택**합니다. <br>
$Y$의 각 feature의 평균을 0로 전처리해주면 ($\forall_i \ \mu_{\mathbf{y_i}} = 0$) $C_Y$는 다음과 같이 계산할 수 있습니다.
$$
C_Y = \frac{1}{m-1}YY' = \frac{1}{m-1} (XP)(XP)' = P \frac{X'X}{m-1} P' = P \ C_X \ P' = diagonal \ matrix
$$

<br>
**cf. Covariance matrix** <br>
- $X$ = [# samples, # dim] = [n, m] → $C_X = \frac{1}{m-1}X'X \in \mathbb{n, n}$ <br>
- $X$ = [# dim, # samples] = [m, n] → $C_X = \frac{1}{m-1}XX' \in \mathbb{n, n}$

<br>3. **$C_X$의 spectral decomposition**을 통해, $C_Y$는 다음과 같이 대각화됩니다. <br>
$ Q' \ C_X \ Q = \Sigma $
- $Q$: $C_X$의 eigenmatrix
- $ \Sigma $: $C_X$의 eigenvalues
$ C_Y = P (Q \Sigma Q') P' $ <br>
**Let $P = Q' \quad \cdots \quad PQ = I \ (\because \ Q \ is \ orthogonal)$** <br>
$ P \in \mathbb{R}^{n \times k}, Q \in \mathbb{R}^{k \times n} \ (n ≥ k)$
$C_Y = \Sigma $ <br>
즉, $C_X$의 eigenvalues가 분산이 됩니다.

<br>4. **Spectral theorem**으로 $C_X = \sum_i^n \sigma_i \mathbf{q_i}\mathbf{q_i'}$ 으로 나타날 수 있습니다. <br>
여기서 $\sigma_i$는 위에서 구한 분산에 해당하므로 1 ~ $n$ 사이에서 적절한 값($k$)을 골라 차원을 축소시킬 수 있습니다. <br>
예를 들어 $\frac{\sigma_1 + \sigma_2}{\sigma_1 + \cdots + \sigma_n} = 0.9$ 인 경우 총 분산의 90%에 해당하는 PC들로 차원을 축소시키고 싶다면, 두 개의 PC ($\mathbf{q_1}, \mathbf{q_2}$)만으로 데이터를 표현할 수 있습니다.
