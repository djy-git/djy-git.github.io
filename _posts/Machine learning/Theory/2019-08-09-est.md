---
layout: article
title: Sklearn estimator API
tags: Theory
sidebar:
  nav: docs-en
aside:
  toc: True
---

# Linear regression


# Ensemble model
## Voting classifier

{% hightlight python linenos %}
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC


log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier()
svm_clf = SVC()

voting_clf = VotingClassifier(
  estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
  voting='hard')  # estimator들이 predict_proba() 지원 시 voting='soft' 가능
voting_clf.fit(X_train, y_train)
{% endhighlight %}


## Bagging

{% highlight python linenos %}
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier


bag_clf = BaggingClassifier(
  DecisionTreeClassifier(), n_estimators=500,
  max_samples=100, bootstrap=True, n_jobs=-1, oob_score=True)  # bootstrap=False → Pasting
bag_clf.fit(X_train, y_train)
bag_clf.oob_score_  # out-of-bag samples score (accuracy)

{% endhighlight %}

## Random forest

{% highlight python linenos %}
from sklearn.ensemble import RandomForestClassifier

rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)
rnd_clf.fit(X_train, y_train)
for feat, score in zip(features, rnd_clf.feature_importances_):
  print(name, score)
{% endhighlight %}


## AdaBoost
{% highlight python linenos %}
from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), n_estimators=200, learning_rate=0.5)
ada_clf.fit(X_train, y_train)
{% endhighlight %}
