---
layout: article
title: Sklearn estimator API
tags: Theory
sidebar:
  nav: docs-en
aside:
  toc: True
---

<!--more-->

# Linear regression


# Ensemble model
## Voting classifier

{% highlight python linenos %}
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC


log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier()
svm_clf = SVC()

voting_clf = VotingClassifier(
  estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
  voting='hard')  # estimator들이 predict_proba() 지원 시 voting='soft' 가능
voting_clf.fit(X_train, y_train)
voting_clf.predict(X_test)
{% endhighlight %}


## Bagging

{% highlight python linenos %}
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier


bag_clf = BaggingClassifier(
  DecisionTreeClassifier(), n_estimators=500,
  max_samples=100, bootstrap=True, n_jobs=-1, oob_score=True)  # bootstrap=False → Pasting
bag_clf.fit(X_train, y_train)
bag_clf.oob_score_  # out-of-bag samples score (accuracy)
bag_clf.predict(X_test)

{% endhighlight %}

## Random forest

{% highlight python linenos %}
from sklearn.ensemble import RandomForestClassifier


rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)
rnd_clf.fit(X_train, y_train)
for feat, score in zip(features, rnd_clf.feature_importances_):
  print(name, score)
rnd_clf.predict(X_test)
{% endhighlight %}


## AdaBoost
{% highlight python linenos %}
from sklearn.ensemble import AdaBoostClassifier


ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), n_estimators=200, learning_rate=0.5)
ada_clf.fit(X_train, y_train)
ada_clf.predict(X_test)
{% endhighlight %}

## Gradient boosting
{% highlight python linenos %}
from sklearn.tree import DecisionTreeRegressor


gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)
gbrt.fit(X_train, y_train)
gbrt.predict(X_test, y_test)
{% endhighlight %}
