---
layout: article
title: Linear regression
tags: Theory
sidebar:
  nav: docs-en
---

# Remarks
본 포스팅은 **Hands-On Machine Learning with Scikit-Learn & TensorFlow ([Auérlien Géron](https://github.com/ageron/handson-ml), [박해선(역)](https://github.com/rickiepark/handson-ml), 한빛미디어)** 를 기반으로 작성되었습니다.

<!--more-->

---

# I. Linear regression
## 1. Hypothesis model
$\hat{y} = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n = h_\theta(\mathbf{x}) = \theta^T \bf{x}$
- **$\bf{x}$: feature vector**
- **$\theta$: parameter**
- $n$: **feature**의 개수

## 2. Cost function
$MSE(X, h_\theta) = MSE(\theta) = \frac{1}{m}\sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)})^2 =
\frac{1}{m}\sum_{i=1}^m(\mathbf{x^{(i)}}^T \theta - y^{(i)})^2 $
- $m$: **feature vector**의 개수

## 3. Normal equation
해석적인 방법으로 cost function을 최소화시키는 parameter의 방정식을 **normal equation (정규방정식)**이라 합니다. <br> $\hat{\theta} = (X^TX)^{-1}X^T\bf{y}$
{:.success}
