---
layout: article
title: Linear regression
tags: Theory
sidebar:
  nav: docs-en
---

# Remarks
본 포스팅은 **Hands-On Machine Learning with Scikit-Learn & TensorFlow ([Auérlien Géron](https://github.com/ageron/handson-ml), [박해선(역)](https://github.com/rickiepark/handson-ml), 한빛미디어)** 를 기반으로 작성되었습니다.

<!--more-->

---

# I. Linear regression
## 1. Hypothesis model
$\hat{y} = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n = h_\theta(\mathbf{x}) = \theta^T \bf{x}$
- **$\bf{x}$: feature vector**
- **$\theta$: parameter**
- $n$: **feature**의 개수

<br>
## 2. Cost function
$MSE(X, h_\theta) = MSE(\theta) = \frac{1}{m}\sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)})^2 =
\frac{1}{m}\sum_{i=1}^m(\mathbf{x^{(i)}}^T \theta - y^{(i)})^2 $
- $m$: **feature vector**의 개수

<br>
## 3. Normal equation
**Normal equation** <br> 해석적인 (analytic) 방법으로 cost function을 최소화시키는 parameter의 방정식입니다. <br> $\hat{\theta} = (X^TX)^{-1}X^T\bf{y}$
{:.success}

### - Computational complexity
1. $X^TX = [n+1, m] \cdot [m, n+1] = O(m(n+1)^2)$
2. $(X^TX)^{-1} = [n+1, n+1] = O((n+1)^3)$
3. $(X^TX)^{-1}X^T = [n+1, n+1] \cdot [n+1, m] = O(m(n+1)^2)$
4. $(X^TX)^{-1}X^T\mathbf{y} = [n+1, m] \cdot [m, 1] = O(m(n+1))$
$\therefore \ O(n^3) + O(mn^2) = O(mn^2) \ (∵ m > n)$

그러나 *sklearn*에 구현된 `LinearRegression`은 $O(m^{0.72}n^{1.3})$ 정도의 복잡도를 가집니다. [^1]

<br>
## 4. Gradient Descent
**Gradient descent** <br> Cost function을 최소화시키기 위해 parameter에 대한 cost function의 gradient ($\nabla_\theta J(\theta)$)의 역방향으로 반복적으로 parameter를 이동시키는 방법입니다. 여러 종류의 문제에서 최적의 해법을 찾을 수 있어 범용적으로 사용할 수 있는 수치적인 (numerical) 최적화 알고리즘입니다. Feature의 개수에 민감하지 않다는 장점이 있습니다.
{:.success}

Linear regression (뿐만 아니라 polynomial regression[^2])의 cost function은 이차 초곡면 (quadric)[^3] 중에서도 포물면 (paraboloid) 형태의 convex function이기 때문에 적절한 learning rate를 사용하면 gradient descent가 global minima에 가깝게 이동한다는 것이 보장되어 있습니다. <br>

Gradient descent를 사용하기 전에 먼저 확인해야할 점이 있는데 바로 **각 feature의 scale**입니다. <br>
만약 각 feature의 scale의 차이가 크다면, 해당하는 parameter의 learning rate 역시 차이가 있어야 합니다. <br> 예를 들어, feature 2의 scale이 feature 1 보다 작은 경우, cost function에 동일한 변화를 주기 위해선 parameter 1보다 2이 더 크게 변해야 합니다. (cost function graph는 parameter 2 축의 방향으로 길쭉한 모양이 됩니다) 그러나 실제 학습은 parameter 1과 2 모두 동일한 learning rate로 이루어지기 때문에 parameter는 각 feature의 scale에 해당하는 속도로 움직이고 결국 비효율적인 궤적을 그리며 이동하게 됩니다. <br>

![Image](https://miro.medium.com/max/1400/1*ImvekfhM6sXo2IyAdslKLg.png){:.border} [^4]

$ \frac{\partial}{\partial \theta_j} J(\mathbf{\theta}) = \frac{2}{m} \sum_{i=1}^m(\mathbf{x^{(i)}}^T \theta - y^{(i)}) x^{(i)}_j $ <br> $\theta_j$에 대한 편도함수에 $x_j^{(i)}$가 곱해져 있기 때문에 $\mathbf{x}_j$의 scale이 다른 feature와 차이가 날수록 수렴속도 또한 차이가 나게됩니다.
{:.info} <br>

따라서, gradient descent를 사용하기 전에 반드시 `StandardScaler`, `normalize` 등을 통해 모든 feature가 동일한 scale을 갖도록 만들어야 합니다. 그렇지 않으면 수렴하는 데 훨씬 많은 시간이 소모됩니다.


### 1) Batch Gradient Descent
Parameter를 이동시키는 한 step 마다 $\nabla_\theta J(\theta)$ 를 계산하게되는데 이때, 전체 train data를 전부 사용하여 계산하는 gradient descent 방법을 **batch gradient descent**라고 합니다. <br><br>

$ \nabla_\theta J(\theta) = \frac{2}{m} X^T(X\theta - \mathbf{y}) $
#### - Complexity
1. $ X\theta = [m, n+1] \cdot [n+1, 1] = O(m(n+1)) $
2. $ X\theta - \mathbf{y} = [m, 1] - [m, 1] = O(m) $
3. $ \frac{2}{m} X^T(X\theta - \mathbf{y}) = [n+1, m] \cdot [m, 1] = O(m(n+1)) $
$\therefore \ O(mn) $

사용되는 데이터의 개수가 많을수록 gradient를 계산하는데 걸리는 시간이 길어집니다.


---

[^1]: [https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/](https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/)

[^2]: [https://medium.com/100-shades-of-machine-learning/linear-regression-a-tale-of-a-transform-7e050cddbf17](https://medium.com/100-shades-of-machine-learning/linear-regression-a-tale-of-a-transform-7e050cddbf17)

[^3]: [https://ko.wikipedia.org/wiki/%EC%9D%B4%EC%B0%A8_%EC%B4%88%EA%B3%A1%EB%A9%B4](https://ko.wikipedia.org/wiki/%EC%9D%B4%EC%B0%A8_%EC%B4%88%EA%B3%A1%EB%A9%B4)

[^4]: [https://www.coursera.org/learn/machine-learning?source=post_page---------------------------](https://www.coursera.org/learn/machine-learning?source=post_page---------------------------)
