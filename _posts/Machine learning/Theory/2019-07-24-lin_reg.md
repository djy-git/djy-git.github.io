---
layout: article
title: Linear regression
tags: Theory
sidebar:
  nav: docs-en
---

# Remarks
본 포스팅은 **Hands-On Machine Learning with Scikit-Learn & TensorFlow ([Auérlien Géron](https://github.com/ageron/handson-ml), [박해선(역)](https://github.com/rickiepark/handson-ml), 한빛미디어)** 를 기반으로 작성되었습니다.

<!--more-->

---

# I. Linear regression
## 1. Hypothesis model
$\hat{y} = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n = h_\theta(\mathbf{x}) = \theta^T \bf{x}$
- **$\bf{x}$: feature vector**
- **$\theta$: parameter**
- $n$: **feature**의 개수

## 2. Cost function
$MSE(X, h_\theta) = MSE(\theta) = \frac{1}{m}\sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)})^2 =
\frac{1}{m}\sum_{i=1}^m(\mathbf{x^{(i)}}^T \theta - y^{(i)})^2 $
- $m$: **feature vector**의 개수

## 3. Normal equation
해석적인 방법으로 cost function을 최소화시키는 parameter의 방정식을 **normal equation (정규방정식)**이라 합니다. <br> $\hat{\theta} = (X^TX)^{-1}X^T\bf{y}$
{:.success}

### - Computational complexity
1. $X^TX = [n+1, m] \cdot [m, n+1] = O(m(n+1)^2)$
2. $(X^TX)^{-1} = [n+1, n+1] = O((n+1)^3)$
3. $(X^TX)^{-1}X^T = [n+1, n+1] \cdot [n+1, m] = O(m(n+1)^2)$
4. $(X^TX)^{-1}X^T\mathbf{y} = [n+1, m] \cdot [m, 1] = O(m(n+1))$
$\therefore \ O(n^3) + O(mn^2) = O(mn^2) \ (∵ m > n)$

그러나 *sklearn*에 구현된 `LinearRegression`은 $O(m^{0.72}n^{1.3})$ 정도의 복잡도를 가집니다. [^1]




[^1]: [https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/](https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/)
