---
layout: article
title: Linear regression
tags: Theory
sidebar:
  nav: docs-en
---

# Remarks
본 포스팅은 **Hands-On Machine Learning with Scikit-Learn & TensorFlow ([Auérlien Géron](https://github.com/ageron/handson-ml), [박해선(역)](https://github.com/rickiepark/handson-ml), 한빛미디어)** 를 기반으로 작성되었습니다.

<!--more-->

---

# I. Linear regression
## 1. Hypothesis model
$\hat{y} = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n = h_\theta(\mathbf{x}) = \theta^T \bf{x}$
- **$\bf{x}$: feature vector**
- **$\theta$: parameter**
- $n$: **feature**의 개수

<br>
## 2. Cost function
$MSE(X, h_\theta) = MSE(\theta) = \frac{1}{m}\sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)})^2 =
\frac{1}{m}\sum_{i=1}^m(\mathbf{x^{(i)}}^T \theta - y^{(i)})^2 $
- $m$: **feature vector**의 개수

<br>
## 3. Normal equation
**Normal equation** <br> 해석적인 (analytic) 방법으로 cost function을 최소화시키는 parameter의 방정식입니다. <br> $\hat{\theta} = (X^TX)^{-1}X^T\bf{y}$
{:.success}

### - Computational complexity
1. $X^TX = [n+1, m] \cdot [m, n+1] = O(m(n+1)^2)$
2. $(X^TX)^{-1} = [n+1, n+1] = O((n+1)^3)$
3. $(X^TX)^{-1}X^T = [n+1, n+1] \cdot [n+1, m] = O(m(n+1)^2)$
4. $(X^TX)^{-1}X^T\mathbf{y} = [n+1, m] \cdot [m, 1] = O(m(n+1))$
$\therefore \ O(n^3) + O(mn^2) = O(mn^2) \ (∵ m > n)$

그러나 *sklearn*에 구현된 `LinearRegression`은 $O(m^{0.72}n^{1.3})$ 정도의 복잡도를 가집니다. [^1]

<br>
## 4. Gradient descent
**Gradient descent** <br> Cost function을 최소화시키기 위해 parameter에 대한 cost function의 gradient ($\nabla_\theta  L(\theta)$)의 역방향으로 반복적으로 parameter를 이동시키는 방법입니다. 여러 종류의 문제에서 최적의 해법을 찾을 수 있어 범용적으로 사용할 수 있는 수치적인 (numerical) 최적화 알고리즘입니다.
{:.success}

Linear regression (뿐만 아니라 polynomial regression[^2])의 cost function은 이차 초곡면 (quadric)[^3] 중에서도 포물면 (paraboloid) 형태의 convex function이기 때문에 적절한 learning rate를 사용하면 gradient descent가 global minima에 가깝게 이동한다는 것이 보장되어 있습니다. <br>

Gradient descent를 사용하기 전에 먼저 확인해야할 점이 있는데 바로 **각 feature의 scale**입니다. <br>
만약 각 feature의 scale의 차이가 크다면, 해당하는 parameter의 learning rate 역시 차이가 있어야 합니다. <br> 예를 들어, feature 1의 scale이 feature 2 보다 작은 경우, cost function에 동일한 변화를 주기 위해선 parameter 2보다 1이 더 크게 변해야 합니다. (cost function graph는 parameter 1 축의 방향으로 길쭉한 모양이 됩니다) <br>

$ \frac{\partial}{\partial \theta_j} MSE(\mathbf{\theta}) = \frac{2}{m} \sum_{i=1}^m(\mathbf{x^{(i)}}^T \theta - y^{(i)}) x^{(i)}_j $ <br> $\theta_j$에 대한 편미분값 (기울기)에 $x_j^{(i)}$가 곱해져 있기 때문에 $\mathbf{x}_j$의 scale이 다른 feature와 차이가 날수록 수렴속도 또한 차이가 나게됩니다.
{:.info}

그러나 실제론 parameter 1과 2 모두 동일한 learning rate로 학습시키기 때문에, 각 parameter에 대해 비슷한 속도로 움직여 직선에 가까운 궤적으로 이동하지 않기 때문에 더 많은 시간이 걸리게 됩니다. <br>
따라서, gradient descent를 사용하기 전에는 반드시 `StandardScaler` 등을 사용하여 모든 feature가 같은 scale을 갖도록 만들어야 합니다. 그렇지 않으면 수렴하는 데 훨씬 많은 시간이 소모됩니다.



[^1]: [https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/](https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/)

[^2]: [https://medium.com/100-shades-of-machine-learning/linear-regression-a-tale-of-a-transform-7e050cddbf17](https://medium.com/100-shades-of-machine-learning/linear-regression-a-tale-of-a-transform-7e050cddbf17)

[^3]: [https://ko.wikipedia.org/wiki/%EC%9D%B4%EC%B0%A8_%EC%B4%88%EA%B3%A1%EB%A9%B4](https://ko.wikipedia.org/wiki/%EC%9D%B4%EC%B0%A8_%EC%B4%88%EA%B3%A1%EB%A9%B4)
