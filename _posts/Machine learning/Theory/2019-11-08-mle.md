---
layout: article
title: Probabilistic Estimation
tags: Theory
sidebar:
  nav: docs-en
aside:
  toc: true
---

<!--more-->

## 1. Maximum Likelihood Estimation (ML, MLE)
### 1) Notations
- **Data** <br>
(Input, Output) : $ (x_{1..S}, \ y_{1..S}) $

- **Set of parameters** <br>
$\theta$

- **Model** <br>
$ f_\theta(x_i) $

- **Assumptions** <br>
i.i.d. Condition: Independent and Identically Distributed


### 2) Likelihood (function)
- ***Definition*** <br>
A function that expresses the probability of a sample of data given a set of model parameter values

- **Single data** <br>
$ L(\theta | x_i) = p_\theta(x_i) = p(x_i | \theta) $ <br>
$ p(y_i | f_\theta(x_i)) $

- **Multiple data** <br>
$ L(\theta) = \Pi_i p(y_i | f_\theta(x_i)) $

- **Log likelihood** <br>
$ l(\theta) = \sum_i log[p(y_i | f_\theta(x_i))] $


### 3) Maximum Likelihood Estimation
- ***Definition*** <br>
A method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable

- **Maximum Likelihood Estimate** <br>
The point in the parameter space that maximizes the likelihood function

- **Univariate cases** <br>
  - **Gaussian distribution** <br>
$$
\tilde{y_i} = y_i | \mu_i, \sigma^2_i \ \sim ~ N(\mu_i, \sigma^2_i) \quad s.t. \  \mu_i = f_\theta(x_i), \ \sigma^2_i = 1 \\
p(y_i | \mu_i, \sigma^2_i) = (2\pi\sigma^2)^{-\frac{1}{2}} exp\{ -\frac{1}{2\sigma^2} (y_i - \mu_i)^2 \} \\

-log[p(y_i | \mu_i, \sigma^2)] = \frac{1}{2} log (2\pi \sigma^2) + \frac{1}{2\sigma^2}(y_i - \mu_i)^2 \\

$$
