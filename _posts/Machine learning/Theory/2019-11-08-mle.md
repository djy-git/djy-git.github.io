---
layout: article
title: Probabilistic Estimation
tags: Theory
sidebar:
  nav: docs-en
aside:
  toc: true
---

<!--more-->

## 1. Maximum Likelihood Estimation (ML, MLE)
### 1) Notations
- **Data** <br>
(Input, Output) : $ (x_{1..S}, \ y_{1..S}) $

- **Set of parameters** <br>
$\theta$

- **Model** <br>
$ f_\theta(x_i) $

- **Assumptions** <br>
i.i.d. Condition: Independent and Identically Distributed


### 2) Likelihood (function)
- ***Definition*** <br>
A function that expresses the probability of a sample of data given a set of model parameter values

- **Single data** <br>
$ L(\theta | x_i) = p_\theta(x_i) = p(x_i | \theta) $ <br>
$ p(y_i | f_\theta(x_i)) $

- **Multiple data** <br>
$ L(\theta) = \Pi_i p(y_i | f_\theta(x_i)) $

- **Log likelihood** <br>
$ l(\theta) = \sum_i log[p(y_i | f_\theta(x_i))] $


### 3) Maximum Likelihood Estimation
- ***Definition*** <br>
A method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable

- **Maximum Likelihood Estimate** <br>
The point in the parameter space that maximizes the likelihood function

- **Univariate cases** <br>
  - **Gaussian distribution** <br>
  $$
  \tilde{y_i} = y_i | \mu_i, \sigma^2_i \ \sim ~ N(\mu_i, \sigma^2_i) \quad s.t. \  \color{red}{\mu_i = f_\theta(x_i)}, \ \sigma^2_i = 1 \\
  \begin{equation}
  \begin{aligned}
    p(y_i | \mu_i, \sigma^2_i) &= (2\pi\sigma^2_i)^{-\frac{1}{2}} exp\{ -\frac{1}{2\sigma^2_i} (y_i - \mu_i)^2 \} \\
    -log[p(y_i | \mu_i, \sigma^2_i)] &= \frac{1}{2} log (2\pi \sigma^2_i) + \frac{1}{2\sigma^2_i}(y_i - \mu_i)^2 \\
    &∝ \frac{1}{2}(y_i - \mu_i)^2 \\
    &= \color{blue}{\frac{1}{2}(y_i - f_\theta(x_i))^2}
  \end{aligned}
  \end{equation}
  $$
  Maximize Likelihood → Minimize Mean Squared Error (MSE)

  - **Bernoulli distribution** <br>
  $$
  \tilde{y_i} = y_i | p_i \ \sim ~ Ber(p_i) \quad s.t. \  \color{red}{p_i = f_\theta(x_i)} \\
  \begin{equation}
  \begin{aligned}
    p(y_i | p_i) &= p^{y_{i}}_i(1 - p_i)^{1 - y_i} \\
    log[p(y_i | p_i)] &= y_i log p_i + (1 - y_i)log(1 - p_i) \\
    -log[p(y_i | p_i)] &= \color{blue}{-[y_i log p_i + (1 - y_i)log(1 - p_i)]} \\
  \end{aligned}
  \end{equation}
  $$
  Maximize Likelihood → Minimize Cross Entropy Error (CEE)
