---
layout: article
title: Ensemble
tags: Theory
sidebar:
  nav: docs-en
aside:
  toc: True
---

**Ensemble** <br> 일련의 예측기로부터 예측을 수집하면 가장 좋은 한 가지 모델보다 더 좋은 예측을 얻을 수 있습니다. <br> 일련의 예측기를 **ensemble**이라고 부르고, 이러한 방식으로 학습하는 것을  **ensemble learning**, ensemble learning algorithm을 **ensemble method**라고 부릅니다.
{:.success}

<!--more-->

# Remarks
본 포스팅은 **Hands-On Machine Learning with Scikit-Learn & TensorFlow ([Auérlien Géron](https://github.com/ageron/handson-ml), [박해선(역)](https://github.com/rickiepark/handson-ml), 한빛미디어)** 를 기반으로 작성되었습니다.

---

# 1. Voting classifiers
- Hard voting
각 분류기가 예측하는 class를 모아 가장 많이 선택된 class로 예측하는 방법
- Soft voting
각 분류기가 예측하는 class의 확률을 평균내어 가장 높은 확률의 class로 예측하는 방법으로, <br>
더 정확하지만 class 확률을 추정할 수 있는 경우(predict_proba())에만 사용이 가능하다.

{% highlight python linenos %}
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score


log_clf = LogisticRegression(solver="liblinear", random_state=42)
rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)
svm_clf = SVC(gamma="auto", probability=True, random_state=42)  # probability=True 하면 속도는 느리지만 soft voting이 가능

voting_clf = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
    voting='soft')  # voting='hard'
voting_clf.fit(X_train, y_train)

for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
{% endhighlight %}
