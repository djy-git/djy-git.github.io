---
layout: article
title: Ensemble
tags: Theory
sidebar:
  nav: docs-en
aside:
  toc: True
---

**Ensemble** <br> 일련의 예측기로부터 예측을 수집하면 가장 좋은 한 가지 모델보다 더 좋은 예측을 얻을 수 있습니다. <br> 일련의 예측기를 **ensemble**이라고 부르고, 이러한 방식으로 학습하는 것을  **ensemble learning**, ensemble learning algorithm을 **ensemble method**라고 부릅니다.
{:.success}

<!--more-->

# Remarks
본 포스팅은 **Hands-On Machine Learning with Scikit-Learn & TensorFlow ([Auérlien Géron](https://github.com/ageron/handson-ml), [박해선(역)](https://github.com/rickiepark/handson-ml), 한빛미디어)** 를 기반으로 작성되었습니다.

---

# 1. Voting classifiers
- **Hard voting** <br>
각 분류기가 예측하는 class를 모아 가장 많이 선택된 class로 예측하는 방법
- **Soft voting** <br>
각 분류기가 예측하는 class의 확률을 평균내어 가장 높은 확률의 class로 예측하는 방법으로, <br>
더 정확하지만 class 확률을 추정할 수 있는 경우(`predict_proba()`)에만 사용이 가능합니다.

{% highlight python linenos %}
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score


log_clf = LogisticRegression(solver="liblinear", random_state=42)
rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)
svm_clf = SVC(gamma="auto", probability=True, random_state=42)  # probability=True 하면 속도는 느리지만 soft voting이 가능

voting_clf = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
    voting='soft')  # voting='hard'
voting_clf.fit(X_train, y_train)

for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
{% endhighlight %}

<br>
# 2. Bagging and pasting
- **Bagging (Bootstrap aggregating)** <br>
학습 데이터에서 중복을 허용하고 샘플링하여 분류기를 각기 다르게 학습시키고 voting 혹은 평균을 내어 예측하는 방법 <br>
각 예측기가 학습하는 subset에 다양성을 증가시키기 때문에 ($_n\Pi_r > \ _nC_r$) bagging이 pasting보다 bias가 조금 더 높지만, 예측기들의 상관관계를 줄이므로 ensemble의 variance를 감소시킵니다. 일반적으로 pasting보다 bagging을 더 선호합니다.
- **Pasting** <br>
학습 데이터에서 중복을 허용하지 않고 샘플링하여 분류기를 각기 다르게 학습시키고 voting 혹은 평균을 내어 예측하는 방법

*Sklearn*의 `BaggingClassifier`는 `predict_proba()`가 존재하면 soft voting으로 수행합니다.

{% highlight python linenos %}

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier


bag_clf = BaggingClassifier(
    DecisionTreeClassifier(random_state=42), n_estimators=500,
    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)

{% endhighlight %}


## oob (Out-of-Bag) evaluation
Sample의 크기를 총 학습 데이터와 동일하게 사용하는 bagging의 경우, 학습 데이터의 약 63.2% 만을 학습에 사용합니다. (.632 bootstrap) <br>
선택되지 않은 나머지 약 37% 데이터를 **oob (out-of-bag)** sample이라고 부릅니다. <br>

oob sample은 예측기가 보지 못한 데이터이기 때문에 이들에 대한 score로 ensemble을 평가할 수 있습니다. <br>

{% highlight python linenos %}
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(random_state=42), n_estimators=500,
    bootstrap=True, n_jobs=-1, oob_score=True, random_state=40)
bag_clf.fit(X_train, y_train)
bag_clf.oob_score_
{% endhighlight %}


{% highlight python linenos %}
{% endhighlight %}

{% highlight python linenos %}
{% endhighlight %}

{% highlight python linenos %}
{% endhighlight %}

{% highlight python linenos %}
{% endhighlight %}

{% highlight python linenos %}
{% endhighlight %}

{% highlight python linenos %}
{% endhighlight %}

{% highlight python linenos %}
{% endhighlight %}

{% highlight python linenos %}
{% endhighlight %}

{% highlight python linenos %}
{% endhighlight %}

{% highlight python linenos %}
{% endhighlight %}

{% highlight python linenos %}
{% endhighlight %}

{% highlight python linenos %}
{% endhighlight %}
